{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "\n",
    "import dotenv\n",
    "import litellm\n",
    "\n",
    "litellm.suppress_debug_info = True\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def debug_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    print(example.inputs())\n",
    "    print(example.answer)\n",
    "    print(pred)\n",
    "    # print(trace)\n",
    "    # print(frac)\n",
    "    return answer_exact_match(example, pred, trace, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "# vllm serve Qwen/Qwen2-VL-7B-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --pipeline-parallel-size 2\n",
    "qwen_lm = dspy.LM(model=\"openai/Qwen/Qwen2-VL-7B-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "haiku_lm = dspy.LM(model=\"anthropic/claude-3-haiku-20240307\", max_tokens=4096)\n",
    "# vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --enforce-eager --max-num-seqs 48\n",
    "llama_lm = dspy.LM(model=\"openai/meta-llama/Llama-3.2-11B-Vision-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "internlm_lm = dspy.LM(model=\"openai/OpenGVLab/InternVL2-8B\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "gpt_lm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=5000)\n",
    "all_lms = [qwen_lm, haiku_lm, llama_lm, gpt_lm]\n",
    "\n",
    "dspy.settings.configure(lm=gpt_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "cwd = os.getcwd()\n",
    "# os.environ.pop(\"HF_HOME\", None)\n",
    "# os.environ[\"HF_HOME\"] = f\"{cwd}/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73338f263834430a3de8a28969a1c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 134 files:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/datasets--yubo2333--MMLongBench-Doc/snapshots/38bceac8784469e70ad783dbf26c0b6ff08e0a9a\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This download is flaky\n",
    "# Just keep trying it and deleting .cache until it works\n",
    "# Sometimes adding: ignore_patterns=\"*mi_phone*\", helps\n",
    "documents_path = snapshot_download(\"yubo2333/MMLongBench-Doc\", allow_patterns=\"documents/*\", ignore_patterns=\"*mi_phone*\", repo_type=\"dataset\", max_workers=1)\n",
    "print(documents_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'doc_id': 'PH_2016.06.08_Economy-Final.pdf', 'doc_type': 'Research report / Introduction', 'question': 'According to the report, how do 5% of the Latinos see economic upward mobility for their children?', 'answer': 'Less well-off', 'evidence_pages': '[5]', 'evidence_sources': \"['Chart']\", 'answer_format': 'Str'}) (input_keys=set())\n"
     ]
    }
   ],
   "source": [
    "dataset = DataLoader().from_huggingface(\"yubo2333/MMLongBench-Doc\")\n",
    "\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index mmlongbench-doc-nielsen2015musicbizpresentation-final-150526143534-lva1-app6891_95 already exists\n",
      "Index mmlongbench-doc-disciplined-agile-business-analysis-160218012713_95 already exists\n",
      "Index mmlongbench-doc-nova_y70 already exists\n",
      "Index mmlongbench-doc-STEPBACK already exists\n",
      "Index mmlongbench-doc-efd88e41c5f2606c57929cac6c1c0605 already exists\n",
      "Index mmlongbench-doc-csewt7zsecmmbzjufbyx-signature-24d91a254426c21c3079384270e1f138dc43a271cfe15d6d520d68205855b2a3-poli-150306115347-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-COSTCO_2021_10K already exists\n",
      "Index mmlongbench-doc-formwork-150318073913-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-germanwingsdigitalcrisisanalysis-150403064828-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-PP_2019.01.17_Trump-economy_FINAL2 already exists\n",
      "Index mmlongbench-doc-AMAZON_2017_10K already exists\n",
      "Index mmlongbench-doc-a4f3ced0696009fec3179f493e4f28c4 already exists\n",
      "Index mmlongbench-doc-2311.16502v3 already exists\n",
      "Index mmlongbench-doc-indonesiamobilemarketresearch-ag-150106055934-conversion-gate02_95 already exists\n",
      "Index mmlongbench-doc-2405.09818v1 already exists\n",
      "Index mmlongbench-doc-welcome-to-nus already exists\n",
      "Index mmlongbench-doc-3276a5b991c49cf5f9a4af0f7d6fce67 already exists\n",
      "Index mmlongbench-doc-PG_2021.03.04_US-Views-on-China_FINAL already exists\n",
      "Index mmlongbench-doc-DSA-278777 already exists\n",
      "Index mmlongbench-doc-936c0e2c2e6c8e0c07c51bfaf7fd0a83 already exists\n",
      "Index mmlongbench-doc-7c3f6204b3241f142f0f8eb8e1fefe7a already exists\n",
      "Index mmlongbench-doc-PH_2016.06.08_Economy-Final already exists\n",
      "Index mmlongbench-doc-PIP_Seniors-and-Tech-Use_040314 already exists\n",
      "Index mmlongbench-doc-Macbook_air already exists\n",
      "Index mmlongbench-doc-honor_watch_gs_pro already exists\n",
      "Index mmlongbench-doc-2023.acl-long.386 already exists\n",
      "Index mmlongbench-doc-f8d3a162ab9507e021d83dd109118b60 already exists\n",
      "Index mmlongbench-doc-05-03-18-political-release already exists\n",
      "Index mmlongbench-doc-11-21-16-Updated-Post-Election-Release already exists\n",
      "Index mmlongbench-doc-Sinopolis-Chengdu already exists\n",
      "Index mmlongbench-doc-owners-manual-2170416 already exists\n",
      "Index mmlongbench-doc-caltraincapacitymountainview1-150701205750-lva1-app6891_95 already exists\n",
      "Index mmlongbench-doc-2312.09390v1 already exists\n",
      "Index mmlongbench-doc-BESTBUY_2023_10K already exists\n",
      "Index mmlongbench-doc-User_Manual_1500S_Classic_EN already exists\n",
      "Index mmlongbench-doc-2310.09158v1 already exists\n",
      "Index mmlongbench-doc-fd76bbefe469561966e5387aa709c482 already exists\n",
      "Index mmlongbench-doc-ADOBE_2015_10K already exists\n",
      "Index mmlongbench-doc-ddoseattle-150627210357-lva1-app6891_95 already exists\n",
      "Index mmlongbench-doc-PS_2018.01.09_STEM_FINAL already exists\n",
      "Index mmlongbench-doc-chapter8-geneticscompatibilitymode-141214140247-conversion-gate02_95 already exists\n",
      "Index mmlongbench-doc-measuringsuccessonfacebooktwitterlinkedin-160317142140_95 already exists\n",
      "Index mmlongbench-doc-amb-siteaudits-ds15-150204174043-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-dr-vorapptchapter1emissionsources-121120210508-phpapp02_95 already exists\n",
      "Index mmlongbench-doc-2024.ug.eprospectus already exists\n",
      "Index mmlongbench-doc-PG_20.07.30_U.S.-Views-China_final already exists\n",
      "Index mmlongbench-doc-finalmediafindingspdf-141228031149-conversion-gate02_95 already exists\n",
      "Index mmlongbench-doc-watch_d already exists\n",
      "Index mmlongbench-doc-3M_2018_10K already exists\n",
      "Index mmlongbench-doc-b3m5kaeqm2w8n4bwcesw-140602121350-phpapp02_95 already exists\n",
      "Index mmlongbench-doc-91521110100M_4K_UHD_Display_User_Manual_V1.1 already exists\n",
      "Index mmlongbench-doc-2023.findings-emnlp.248 already exists\n",
      "Index mmlongbench-doc-PG_2020.05.21_International-Cooperation-COVID_FINAL already exists\n",
      "Index mmlongbench-doc-PI_2017.10.04_Automation_FINAL already exists\n",
      "Index mmlongbench-doc-asdaaburson-marstellerarabyouthsurvey2014-140407100615-phpapp01_95 already exists\n",
      "Index mmlongbench-doc-camry_ebrochure already exists\n",
      "Index mmlongbench-doc-StudentSupport_Guidebook already exists\n",
      "Index mmlongbench-doc-2401.18059v1 already exists\n",
      "Index mmlongbench-doc-Guide-for-international-students-web already exists\n",
      "Index mmlongbench-doc-e79deb02a0c0e87511080836c5d4347b already exists\n",
      "Index mmlongbench-doc-2310.07609v1 already exists\n",
      "Index mmlongbench-doc-mmdetection-readthedocs-io-en-v2.18.0 already exists\n",
      "Index mmlongbench-doc-bariumswallowpresentation-090810084400-phpapp01_95 already exists\n",
      "Index mmlongbench-doc-PP_2020.08.06_COVID-19-Restrictions_FINAL-1 already exists\n",
      "Index mmlongbench-doc-f86d073b0d735ac873a65d906ba82758 already exists\n",
      "Index mmlongbench-doc-fdac8d1e9ef56519371df7e6532df27d already exists\n",
      "Index mmlongbench-doc-PI_2018.11.19_algorithms_FINAL already exists\n",
      "Index mmlongbench-doc-SnapNTell already exists\n",
      "Index mmlongbench-doc-afe620b9beac86c1027b96d31d396407 already exists\n",
      "Index mmlongbench-doc-NIKE_2021_10K already exists\n",
      "Index mmlongbench-doc-2309.17421v2 already exists\n",
      "Index mmlongbench-doc-NUS-FASS-Graduate-Guidebook-2021-small already exists\n",
      "Index mmlongbench-doc-2307.09288v2 already exists\n",
      "Index mmlongbench-doc-efis-140411041451-phpapp01_95 already exists\n",
      "Index mmlongbench-doc-bdf54dxa already exists\n",
      "Index mmlongbench-doc-Independents-Report already exists\n",
      "Index mmlongbench-doc-guojixueshengshenghuozhinanyingwen9.1 already exists\n",
      "Index mmlongbench-doc-2312.10997v5 already exists\n",
      "Index mmlongbench-doc-e639029d16094ea71d964e2fb953952b already exists\n",
      "Index mmlongbench-doc-reportq32015-151009093138-lva1-app6891_95 already exists\n",
      "Index mmlongbench-doc-SAO-StudentSupport_Guidebook-Content already exists\n",
      "Index mmlongbench-doc-0b85477387a9d0cc33fca0f4becaa0e5 already exists\n",
      "Index mmlongbench-doc-q1-2023-bilibili-inc-investor-presentation already exists\n",
      "Index mmlongbench-doc-avalaunchpresentationsthatkickasteriskv3copy-150318114804-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-competitiveoutcomes-091006065143-phpapp01_95 already exists\n",
      "Index mmlongbench-doc-Bergen-Brochure-en-2022-23 already exists\n",
      "Index mmlongbench-doc-NETFLIX_2015_10K already exists\n",
      "Index mmlongbench-doc-a5879805d70c854ea4361e43a84e3bb2 already exists\n",
      "Index mmlongbench-doc-bigdatatrends-120723191058-phpapp02_95 already exists\n",
      "Index mmlongbench-doc-Campaign_038_Introducing_AC_Whitepaper_v5e already exists\n",
      "Index mmlongbench-doc-earlybird-110722143746-phpapp02_95 already exists\n",
      "Index mmlongbench-doc-8dfc21ec151fb9d3578fc32d5c4e5df9 already exists\n",
      "Index mmlongbench-doc-edb88a99670417f64a6b719646aed326 already exists\n",
      "Index mmlongbench-doc-ISEP_student_handbook_2020 already exists\n",
      "Index mmlongbench-doc-NYU_graduate already exists\n",
      "Index mmlongbench-doc-ACTIVISIONBLIZZARD_2019_10K already exists\n",
      "Index mmlongbench-doc-ecommerceopportunityindia-141124010546-conversion-gate01_95 already exists\n",
      "Index mmlongbench-doc-c31e6580d0175ab3f9d99d1ff0bfa000 already exists\n",
      "Index mmlongbench-doc-0e94b4197b10096b1f4c699701570fbf already exists\n",
      "Index mmlongbench-doc-2303.05039v2 already exists\n",
      "Index mmlongbench-doc-PP_2021.04.22_voting-access_REPORT already exists\n",
      "Index mmlongbench-doc-8e7c4cb542ad160f80fb3d795ada35d8 already exists\n",
      "Index mmlongbench-doc-GPL-Graduate-Studies-Professional-Learning-Brochure-Jul-2021 already exists\n",
      "Index mmlongbench-doc-2312.04350v3 already exists\n",
      "Index mmlongbench-doc-PRE_2022.09.29_NSL-politics_REPORT already exists\n",
      "Index mmlongbench-doc-digitalmeasurementframework22feb2011v6novideo-110221233835-phpapp01_95 already exists\n",
      "Index mmlongbench-doc-NUS-Business-School-BBA-Brochure-2024 already exists\n",
      "Index mmlongbench-doc-stereo_headset already exists\n",
      "Index mmlongbench-doc-BRO-GL-MMONEY already exists\n",
      "Index mmlongbench-doc-f1f5242528411b262be447e61e2eb10f already exists\n",
      "Index mmlongbench-doc-transform-software-delivery-with-valueedge-brochure already exists\n",
      "Index mmlongbench-doc-2005.12872v3 already exists\n",
      "Index mmlongbench-doc-PG_2020.03.09_US-Germany_FINAL already exists\n",
      "Index mmlongbench-doc-2210.02442v1 already exists\n",
      "Index mmlongbench-doc-PWC_opportunity_of_lifetime already exists\n",
      "Index mmlongbench-doc-379f44022bb27aa53efd5d322c7b57bf already exists\n",
      "Index mmlongbench-doc-Pew-Research-Center_Hispanic-Identity-Report_12.20.2017 already exists\n",
      "Index mmlongbench-doc-2303.08559v2 already exists\n",
      "Index mmlongbench-doc-catvsdogdlpycon15se-150512122612-lva1-app6891_95 already exists\n",
      "Index mmlongbench-doc-san-francisco-11-contents already exists\n",
      "Index mmlongbench-doc-earthlinkweb-150213112111-conversion-gate02_95 already exists\n",
      "Index mmlongbench-doc-obs-productdesc-en already exists\n",
      "Index mmlongbench-doc-2306.05425v1 already exists\n",
      "Index mmlongbench-doc-2310.05634v2 already exists\n",
      "Index mmlongbench-doc-RAR already exists\n",
      "Index mmlongbench-doc-finalpresentationdeck-whatwhyhowofcertificationsocial-160324220748_95 already exists\n",
      "Index mmlongbench-doc-2021-Apple-Catalog already exists\n",
      "Index mmlongbench-doc-2305.14160v4 already exists\n",
      "Index mmlongbench-doc-2305.13186v3 already exists\n",
      "Index mmlongbench-doc-52b3137455e7ca4df65021a200aef724 already exists\n",
      "Index mmlongbench-doc-698bba535087fa9a7f9009e172a7f763 already exists\n",
      "Index mmlongbench-doc-t480_ug_en already exists\n",
      "Index mmlongbench-doc-12-15-15-ISIS-and-terrorism-release-final already exists\n",
      "Index mmlongbench-doc-tacl_a_00660 already exists\n"
     ]
    }
   ],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "import glob\n",
    "from pathlib import Path\n",
    "# Optionally, you can specify an `index_root`, which is where it'll save the index. It defaults to \".byaldi/\".\n",
    "\n",
    "\n",
    "def create_all_indexes():\n",
    "    index_root = \".byaldi\"\n",
    "    document_paths = glob.glob(f\"{documents_path}/documents/*.pdf\")\n",
    "    for document_path in document_paths:\n",
    "        index_name = f\"mmlongbench-doc-{Path(document_path).stem}\"\n",
    "        index_path = Path(index_root) / Path(index_name)\n",
    "        if index_path.exists():\n",
    "            print(f\"Index {index_name} already exists\")\n",
    "            continue\n",
    "        RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\", verbose=0)\n",
    "        print(f\"Creating index {index_name}\")\n",
    "        RAG.index(\n",
    "            input_path=document_path, # The path to your documents\n",
    "            index_name=index_name, # The name you want to give to your index. It'll be saved at `index_root/index_name/`.\n",
    "            store_collection_with_index=False, # Whether the index should store the base64 encoded documents.\n",
    "            # doc_ids=[0, 1, 2], # Optionally, you can specify a list of document IDs. They must be integers and match the number of documents you're passing. Otherwise, doc_ids will be automatically created.\n",
    "            # metadata=[{\"author\": \"John Doe\", \"date\": \"2021-01-01\"}], # Optionally, you can specify a list of metadata for each document. They must be a list of dictionaries, with the same length as the number of documents you're passing.\n",
    "            overwrite=False, # Whether to overwrite an index if it already exists. If False, it'll return None and do nothing if `index_root/index_name` exists.\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "CREATE_INDEX = True\n",
    "if CREATE_INDEX:\n",
    "    create_all_indexes()\n",
    "# else:\n",
    "#     RAG.from_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(\"images: List[dspy.Image], context: str, question: str -> query: str\") for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context: str, question: str -> answer: str\")\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The first image is a bright green color, and the second image is a deep blue color.',\n",
      "    answer=['green', 'blue']\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2024-11-13T00:24:32.152468]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `images` (list[Image])\n",
      "2. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (list[str])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## images ## ]]\n",
      "{images}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `images`, `question`, produce the fields `answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## images ## ]]\n",
      "[\n",
      "\u001b[34m<image_url: https://placehold.co/300/00FF00/00FF00.png>\u001b[0m\n",
      "\n",
      ",\n",
      "\u001b[34m<image_url: https://placehold.co/300/0000FF/0000FF.png>\u001b[0m\n",
      "\n",
      "]\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What is the color of the images?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The first image is a bright green color, and the second image is a deep blue color. \n",
      "\n",
      "[[ ## answer ## ]]\n",
      "[\"green\", \"blue\"]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "colors = {\n",
    "    \"White\": \"FFFFFF\",\n",
    "    \"Red\": \"FF0000\",\n",
    "    \"Green\": \"00FF00\",\n",
    "    \"Blue\": \"0000FF\",\n",
    "    \"Yellow\": \"FFFF00\",\n",
    "    \"Cyan\": \"00FFFF\",\n",
    "    \"Magenta\": \"FF00FF\",\n",
    "    \"Gray\": \"808080\",\n",
    "    \"Orange\": \"FFA500\",\n",
    "    \"Purple\": \"800080\"\n",
    "}\n",
    "def get_color_image_url(color, file_extension=\"png\"):\n",
    "    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n",
    "\n",
    "green_image = dspy.Image.from_url(get_color_image_url(\"Green\"))\n",
    "blue_image = dspy.Image.from_url(get_color_image_url(\"Blue\"))\n",
    "\n",
    "inputs = {\n",
    "    \"images\": [green_image, blue_image],\n",
    "    \"question\": \"What is the color of the images?\"\n",
    "}\n",
    "image_list_cot = dspy.ChainOfThought(\"images: List[dspy.Image], question: str -> answer: List[str]\")\n",
    "print(image_list_cot(**inputs))\n",
    "gpt_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2024-11-13T00:21:35.001699]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `images` (list[Image])\n",
      "2. `question` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (list[str])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## images ## ]]\n",
      "{images}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `images`, `question`, produce the fields `answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "\u001b[34m<image_url: https://placehold.co/300/00FF00/00FF00.png>\u001b[0m\n",
      "\n",
      "[[ ## images ## ]]\n",
      "[\n",
      "\u001b[34m<image_url: https://placehold.co/300/0000FF/0000FF.png>\u001b[0m\n",
      "\n",
      ", \n",
      "\n",
      "]\n",
      "\n",
      "[[ ## question ## ]]\n",
      "What is the color of the images?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The first image is a bright green color, and the second image is a blue color.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "[\"green\", \"blue\"]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_lm.inspect_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
