{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "\n",
    "import dotenv\n",
    "import litellm\n",
    "\n",
    "litellm.suppress_debug_info = True\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def debug_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    print(example.inputs())\n",
    "    print(example.answer)\n",
    "    print(pred)\n",
    "    # print(trace)\n",
    "    # print(frac)\n",
    "    return answer_exact_match(example, pred, trace, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "# vllm serve Qwen/Qwen2-VL-7B-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --pipeline-parallel-size 2\n",
    "qwen_lm = dspy.LM(model=\"openai/Qwen/Qwen2-VL-7B-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "haiku_lm = dspy.LM(model=\"anthropic/claude-3-haiku-20240307\", max_tokens=4096)\n",
    "# vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --enforce-eager --max-num-seqs 48\n",
    "llama_lm = dspy.LM(model=\"openai/meta-llama/Llama-3.2-11B-Vision-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "internlm_lm = dspy.LM(model=\"openai/OpenGVLab/InternVL2-8B\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "gpt_lm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=5000)\n",
    "all_lms = [qwen_lm, haiku_lm, llama_lm, gpt_lm]\n",
    "\n",
    "dspy.settings.configure(lm=gpt_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "cwd = os.getcwd()\n",
    "# os.environ.pop(\"HF_HOME\", None)\n",
    "# os.environ[\"HF_HOME\"] = f\"{cwd}/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This download is flaky\n",
    "# Just keep trying it and deleting .cache until it works\n",
    "# Sometimes adding: ignore_patterns=\"*mi_phone*\", helps\n",
    "documents_path = snapshot_download(\"yubo2333/MMLongBench-Doc\", allow_patterns=\"documents/*\", ignore_patterns=\"*mi_phone*\", repo_type=\"dataset\", max_workers=1)\n",
    "print(documents_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader().from_huggingface(\"yubo2333/MMLongBench-Doc\")\n",
    "\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_lm.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "import glob\n",
    "from pathlib import Path\n",
    "# Optionally, you can specify an `index_root`, which is where it'll save the index. It defaults to \".byaldi/\".\n",
    "\n",
    "\n",
    "def create_all_indexes(paths=None):\n",
    "    index_root = \".byaldi\"\n",
    "    if not os.path.exists(index_root):\n",
    "        os.makedirs(index_root)\n",
    "\n",
    "    for document_path in paths:\n",
    "        index_name = f\"mmlongbench-doc-{Path(document_path).stem}\"\n",
    "        index_path = Path(index_root) / Path(index_name)\n",
    "        if index_path.exists():\n",
    "            print(f\"Index {index_name} already exists at {index_path}\")\n",
    "            continue\n",
    "        print(f\"Creating index {index_name}\")\n",
    "        RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali-v1.2\", verbose=0)\n",
    "        print(f\"model loaded\")\n",
    "        RAG.index(\n",
    "            input_path=document_path, # The path to your documents\n",
    "            index_name=index_name, # The name you want to give to your index. It'll be saved at `index_root/index_name/`.\n",
    "            store_collection_with_index=False, # Whether the index should store the base64 encoded documents.\n",
    "            # doc_ids=[0, 1, 2], # Optionally, you can specify a list of document IDs. They must be integers and match the number of documents you're passing. Otherwise, doc_ids will be automatically created.\n",
    "            # metadata=[{\"author\": \"John Doe\", \"date\": \"2021-01-01\"}], # Optionally, you can specify a list of metadata for each document. They must be a list of dictionaries, with the same length as the number of documents you're passing.\n",
    "            overwrite=True, # Whether to overwrite an index if it already exists. If False, it'll return None and do nothing if `index_root/index_name` exists.\n",
    "        )\n",
    "\n",
    "import concurrent.futures\n",
    "document_paths = glob.glob(f\"{documents_path}/documents/*.pdf\")\n",
    "print(f\"Creating indexes for {len(document_paths)} documents\")\n",
    "num_threads = 4\n",
    "# chunk_size = (len(document_paths) + num_threads - 1) // num_threads\n",
    "# chunks = [document_paths[i:i + chunk_size] for i in range(0, len(document_paths), chunk_size)]\n",
    "create_all_indexes(document_paths)\n",
    "    # with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    #     executor.map(create_all_indexes, chunks)\n",
    "    # else:\n",
    "    #     RAG.from_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_name = \"BESTBUY_2023_10K.pdf\"\n",
    "index_name = f\"mmlongbench-doc-{document_name.split('.')[0]}\"\n",
    "first_index = RAGMultiModalModel.from_index(index_name)\n",
    "# print(first_index.search(\"What is the color of the images?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_documents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# from pypdf import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "import ast\n",
    "\n",
    "questions_about_index = [x for x in dataset[\"train\"] if x[\"doc_id\"] == Path(index_name.split(\"mmlongbench-doc-\")[1]).stem + \".pdf\"]\n",
    "\n",
    "print(questions_about_index[0])\n",
    "# Lets get a baseline by cheating and putting in the evidence pages as given by the dataset\n",
    "\n",
    "def get_evidence_pages(question, document_path):\n",
    "    question = question.copy()\n",
    "    evidence_pages = question[\"evidence_pages\"]\n",
    "    if document_path not in loaded_documents:\n",
    "        images = convert_from_path(document_path, fmt=\"png\")\n",
    "        loaded_documents[document_path] = images\n",
    "    else:\n",
    "        images = loaded_documents[document_path]\n",
    "    evidence_pages = ast.literal_eval(evidence_pages)\n",
    "    evidence_pages = [int(i) - 1 for i in evidence_pages]\n",
    "    question[\"evidence_page_images_pil\"] = [images[i] for i in evidence_pages]\n",
    "    question[\"images\"] = [dspy.Image.from_PIL(images[i]) for i in evidence_pages]\n",
    "    return question.with_inputs(\"images\", \"question\")\n",
    "\n",
    "\n",
    "document_path = f\"{documents_path}/documents/{document_name}\"\n",
    "questions = [get_evidence_pages(question, document_path) for question in questions_about_index]\n",
    "print(len(questions))\n",
    "# Image(question[\"evidence_page_images_pil\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "\n",
    "class MMLongBenchDocSignature(dspy.Signature):\n",
    "    images: List[dspy.Image] = dspy.InputField()\n",
    "    question: str = dspy.InputField()\n",
    "    answer: str = dspy.OutputField(desc=\"The answer to the question. This should be as succinct as possible.\")\n",
    "\n",
    "evaluate = dspy.Evaluate(metric=answer_exact_match, num_threads=10, return_outputs=True, devset=questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_description = dspy.ChainOfThought(MMLongBenchDocSignature)\n",
    "\n",
    "# for question in questions:\n",
    "#     answer = image_description(images=question[\"evidence_page_images\"], question=question[\"question\"])\n",
    "#     print(\"question:\", question[\"question\"])\n",
    "#     print(\"Predicted answer:\", answer)\n",
    "#     print(\"Actual answer:\", question[\"answer\"])\n",
    "#     print(\"=\"*100)\n",
    "score, outputs = evaluate(image_description, devset=questions)\n",
    "print(score)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(\"images: List[dspy.Image], context: str, question: str -> query: str\") for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context: str, question: str -> answer: str\")\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    \"White\": \"FFFFFF\",\n",
    "    \"Red\": \"FF0000\",\n",
    "    \"Green\": \"00FF00\",\n",
    "    \"Blue\": \"0000FF\",\n",
    "    \"Yellow\": \"FFFF00\",\n",
    "    \"Cyan\": \"00FFFF\",\n",
    "    \"Magenta\": \"FF00FF\",\n",
    "    \"Gray\": \"808080\",\n",
    "    \"Orange\": \"FFA500\",\n",
    "    \"Purple\": \"800080\"\n",
    "}\n",
    "def get_color_image_url(color, file_extension=\"png\"):\n",
    "    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n",
    "\n",
    "green_image = dspy.Image.from_url(get_color_image_url(\"Green\"))\n",
    "blue_image = dspy.Image.from_url(get_color_image_url(\"Blue\"))\n",
    "\n",
    "inputs = {\n",
    "    \"images\": [green_image, blue_image],\n",
    "    \"question\": \"What is the color of the images?\"\n",
    "}\n",
    "image_list_cot = dspy.ChainOfThought(\"images: List[dspy.Image], question: str -> answer: List[str]\")\n",
    "print(image_list_cot(**inputs))\n",
    "gpt_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_lm.inspect_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
