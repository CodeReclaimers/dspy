{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: This is not a cookbook\n",
    "\n",
    "This is a testing notebook in order to make sure that multimodal works.\n",
    "\n",
    "Cookbook is on the way, but if you have particular ideas, message @isaac on discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from typing import List\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "import dotenv\n",
    "import litellm\n",
    "\n",
    "litellm.suppress_debug_info = True\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def debug_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    print(example.inputs())\n",
    "    print(example.answer)\n",
    "    print(pred)\n",
    "    # print(trace)\n",
    "    # print(frac)\n",
    "    return answer_exact_match(example, pred, trace, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm serve Qwen/Qwen2-VL-7B-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --pipeline-parallel-size 2\n",
    "qwen_lm = dspy.LM(model=\"openai/Qwen/Qwen2-VL-7B-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "haiku_lm = dspy.LM(model=\"anthropic/claude-3-haiku-20240307\", max_tokens=4096)\n",
    "# vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --enforce-eager --max-num-seqs 48\n",
    "llama_lm = dspy.LM(model=\"openai/meta-llama/Llama-3.2-11B-Vision-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "internlm_lm = dspy.LM(model=\"openai/OpenGVLab/InternVL2-8B\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "gpt_lm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=5000)\n",
    "all_lms = [qwen_lm, haiku_lm, llama_lm, gpt_lm]\n",
    "\n",
    "dspy.settings.configure(lm=gpt_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogPictureSignature(dspy.Signature):\n",
    "    \"\"\"Answer the question based on the image.\"\"\"\n",
    "    image: dspy.Image = dspy.InputField()\n",
    "    question: str = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "class DogPicture(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.predictor = dspy.ChainOfThought(DogPictureSignature)\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n",
    "\n",
    "dog_picture = DogPicture()\n",
    "\n",
    "example = dspy.Example(image=dspy.Image.from_url(\"https://i.pinimg.com/564x/78/f9/6d/78f96d0314d39a1b8a849005123e166d.jpg\"), question=\"What is the breed of the dog in the image?\").with_inputs(\"image\", \"question\")\n",
    "print(dog_picture(**example.inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_lm.inspect_history()\n",
    "import rich\n",
    "rich.print(qwen_lm.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dspy.Predict(\"question -> answer\")\n",
    "p(question=\"What is the capital of France?\")\n",
    "qwen_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "input_keys = tuple([f\"image_{i}\" for i in range(1, 3)] + [\"question\", \"options\"])\n",
    "subsets = ['Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Computer_Science', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Energy_and_Power', 'Finance', 'Geography', 'History', 'Literature', 'Manage', 'Marketing', 'Materials', 'Math', 'Mechanical_Engineering', 'Music', 'Pharmacy', 'Physics', 'Psychology', 'Public_Health', 'Sociology']\n",
    "\n",
    "devset = []\n",
    "valset = []\n",
    "with ThreadPoolExecutor(max_workers=len(subsets)) as executor:\n",
    "    def load_dataset(subset_index_subset):\n",
    "        subset_index, subset = subset_index_subset\n",
    "        dataset = DataLoader().from_huggingface(\"MMMU/MMMU\", subset, split=[\"dev\", \"validation\"], input_keys=input_keys)\n",
    "        return subset_index, dataset[\"dev\"], dataset[\"validation\"]\n",
    "    \n",
    "    results = list(executor.map(load_dataset, enumerate(subsets)))\n",
    "    \n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    for _, dev, val in results:\n",
    "        devset.extend(dev)\n",
    "        valset.extend(val)\n",
    "\n",
    "print(len(devset))\n",
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image counts in devset:\n",
      "0 image(s): 0 examples\n",
      "1 image(s): 137 examples\n",
      "2 image(s): 0 examples\n",
      "3 image(s): 0 examples\n",
      "4 image(s): 0 examples\n",
      "5 image(s): 0 examples\n",
      "\n",
      "Image counts in valset:\n",
      "0 image(s): 0 examples\n",
      "1 image(s): 805 examples\n",
      "2 image(s): 0 examples\n",
      "3 image(s): 0 examples\n",
      "4 image(s): 0 examples\n",
      "5 image(s): 0 examples\n",
      "\n",
      "Multiple choice questions in devset:\n",
      "137 out of 137\n",
      "\n",
      "Multiple choice questions in valset:\n",
      "805 out of 805\n",
      "Example({'id': 'dev_Accounting_1', 'question': 'Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts.', 'options': \"['$63,020', '$58,410', '$71,320', '$77,490']\", 'explanation': '', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>, 'image_2': None, 'image_3': None, 'image_4': None, 'image_5': None, 'image_6': None, 'image_7': None, 'img_type': \"['Tables']\", 'answer': 'D', 'topic_difficulty': 'Easy', 'question_type': 'multiple-choice', 'subfield': 'Financial Accounting', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>}) (input_keys={'image_2', 'image', 'question', 'image_1', 'options'})\n",
      "Example({'id': 'dev_Accounting_1', 'question': 'Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts.', 'options': \"['$63,020', '$58,410', '$71,320', '$77,490']\", 'explanation': '', 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>, 'image_2': None, 'image_3': None, 'image_4': None, 'image_5': None, 'image_6': None, 'image_7': None, 'img_type': \"['Tables']\", 'answer': 'D', 'topic_difficulty': 'Easy', 'question_type': 'multiple-choice', 'subfield': 'Financial Accounting', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>, 'choices': \"['A. $63,020', 'B. $58,410', 'C. $71,320', 'D. $77,490']\"}) (input_keys={'image_2', 'image', 'question', 'image_1', 'choices', 'options'})\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def count_images(dataset):\n",
    "    image_counts = {i: 0 for i in range(6)}  # Initialize counts for 0 to 2 images\n",
    "    for example in dataset:\n",
    "        count = sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None)\n",
    "        image_counts[count] += 1\n",
    "    return image_counts\n",
    "\n",
    "def count_multiple_choice_questions(dataset):\n",
    "    return sum(1 for example in dataset if example[\"question_type\"] == \"multiple-choice\")\n",
    "max_images = 5\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "devset_filtered = [example for example in devset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "valset_filtered = [example for example in valset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "\n",
    "devset_filtered = [example for example in devset_filtered if example[\"question_type\"] == \"multiple-choice\"]\n",
    "valset_filtered = [example for example in valset_filtered if example[\"question_type\"] == \"multiple-choice\"]\n",
    "\n",
    "def update_example_image_key(example):\n",
    "    example_copy = example.copy()\n",
    "    example_copy[\"image\"] = dspy.Image.from_PIL(example_copy[\"image_1\"])\n",
    "    return example_copy.with_inputs(*example.inputs().keys(), \"image\")\n",
    "\n",
    "\n",
    "\n",
    "devset_filtered = list(map(update_example_image_key, devset_filtered))\n",
    "valset_filtered = list(map(update_example_image_key, valset_filtered))\n",
    "\n",
    "devset_image_counts = count_images(devset_filtered)\n",
    "valset_image_counts = count_images(valset_filtered)\n",
    "\n",
    "devset_multiple_choice_questions = count_multiple_choice_questions(devset_filtered)\n",
    "valset_multiple_choice_questions = count_multiple_choice_questions(valset_filtered)\n",
    "\n",
    "print(\"Image counts in devset:\")\n",
    "for count, num_examples in devset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nImage counts in valset:\")\n",
    "for count, num_examples in valset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nMultiple choice questions in devset:\")\n",
    "print(devset_multiple_choice_questions, \"out of\", len(devset_filtered))\n",
    "print(\"\\nMultiple choice questions in valset:\")\n",
    "print(valset_multiple_choice_questions, \"out of\", len(valset_filtered))\n",
    "\n",
    "def convert_multiple_choice_to_letter(dataset):\n",
    "    new_dataset = []\n",
    "    for example in dataset:\n",
    "        if example[\"question_type\"] == \"multiple-choice\":\n",
    "            # print(example[\"options\"])\n",
    "            options = ast.literal_eval(example[\"options\"])\n",
    "            example[\"choices\"] = str([chr(65 + i) + \". \" + option for i, option in enumerate(options)])\n",
    "        else:\n",
    "            example[\"choices\"] = str(ast.literal_eval(example[\"options\"]))\n",
    "            if example[\"choices\"] == []:\n",
    "                example[\"choices\"] = \"Free response\"\n",
    "\n",
    "        updated_example = example.with_inputs(*example.inputs().keys(), \"choices\")\n",
    "        new_dataset.append(updated_example)\n",
    "    return new_dataset\n",
    "\n",
    "print(devset_filtered[0])\n",
    "updated_devset = convert_multiple_choice_to_letter(devset_filtered)\n",
    "print(updated_devset[0])\n",
    "updated_valset = convert_multiple_choice_to_letter(valset_filtered)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "class MMMUSignature(dspy.Signature):\n",
    "    \"\"\"Answer with the letter of the correct answer.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    image: dspy.Image = dspy.InputField()\n",
    "    choices: List[str] = dspy.InputField()\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"] = dspy.OutputField()\n",
    "\n",
    "class MMMUModule(dspy.Module):\n",
    "    def __init__(self, cot=True):\n",
    "        super().__init__()\n",
    "        if cot:\n",
    "            self.predictor = dspy.ChainOfThought(MMMUSignature)\n",
    "        else:\n",
    "            self.predictor = dspy.Predict(MMMUSignature)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        # Clean up predictions\n",
    "        prediction = self.predictor(**kwargs)\n",
    "        # Multiple choice case\n",
    "        if \"A.\" in kwargs[\"choices\"]:\n",
    "            # regex to extract A, B, C, or D, or E\n",
    "            answer = re.search(r'[A-E]', prediction[\"answer\"])\n",
    "            if not answer:\n",
    "                answer = prediction[\"answer\"]\n",
    "            else:\n",
    "                answer = answer.group(0)\n",
    "            prediction[\"answer\"] = answer\n",
    "        # Free response case\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts.', 'options': \"['$63,020', '$58,410', '$71,320', '$77,490']\", 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>, 'image_2': None, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>, 'choices': \"['A. $63,020', 'B. $58,410', 'C. $71,320', 'D. $77,490']\"}) (input_keys={'image_2', 'image', 'question', 'image_1', 'choices', 'options'})\n",
      "field info:  annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'} value:  {question} type:  <class 'str'>\n",
      "field info:  annotation=Image required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Image:', 'desc': '${image}'} value:  {image} type:  <class 'str'>\n",
      "field info:  annotation=List[str] required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Choices:', 'desc': '${choices}'} value:  {choices} type:  <class 'str'>\n",
      "field info:  annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'} value:  {reasoning} type:  <class 'str'>\n",
      "field info:  annotation=Literal['A', 'B', 'C', 'D', 'E'] required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'} value:  {answer}        # note: the value you produce must be one of: A; B; C; D; E type:  <class 'str'>\n",
      "field info:  annotation=NoneType required=True json_schema_extra={'__dspy_field_type': 'output'} value:   type:  <class 'str'>\n",
      "field info:  annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'} value:  Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts. type:  <class 'str'>\n",
      "field info:  annotation=Image required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Image:', 'desc': '${image}'} value:  <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10> type:  <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "field info:  annotation=List[str] required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Choices:', 'desc': '${choices}'} value:  ['A. $63,020', 'B. $58,410', 'C. $71,320', 'D. $77,490'] type:  <class 'str'>\n",
      "Prediction(\n",
      "    reasoning='To determine the missing amounts for company B, we need to analyze the provided image and the context of the question. The image likely contains financial data or calculations that can help us deduce the missing amount. By examining the figures and performing any necessary calculations, we can identify which of the provided choices matches the missing amount.\\n\\nAfter reviewing the image and the choices, the calculations indicate that the missing amount for company B is $63,020, which corresponds to choice A.',\n",
      "    answer='A'\n",
      ")\n",
      "D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2024-11-13T00:56:42.616963]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "2. `image` (Image)\n",
      "3. `choices` (list[str])\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (Literal[A, B, C, D, E])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## image ## ]]\n",
      "{image}\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "{choices}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}        # note: the value you produce must be one of: A; B; C; D; E\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer with the letter of the correct answer.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts.\n",
      "\n",
      "[[ ## image ## ]]\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1234x289 at 0x7921F0091E10>\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "['A. $63,020', 'B. $58,410', 'C. $71,320', 'D. $77,490']\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]` (must be formatted as a valid Python Literal[A, B, C, D, E]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "To determine the missing amounts for company B, we need to analyze the provided image and the context of the question. The image likely contains financial data or calculations that can help us deduce the missing amount. By examining the figures and performing any necessary calculations, we can identify which of the provided choices matches the missing amount.\n",
      "\n",
      "After reviewing the image and the choices, the calculations indicate that the missing amount for company B is $63,020, which corresponds to choice A.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "A\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_input = updated_devset[0]\n",
    "# print(sample_input.inputs())\n",
    "# print(encode_image(sample_input.inputs()[\"image_1\"]))\n",
    "mmmu = MMMUModule()\n",
    "print(sample_input.inputs())\n",
    "print(mmmu(**sample_input.inputs()))\n",
    "print(sample_input.answer)\n",
    "\n",
    "evaluate_mmmu = Evaluate(metric=answer_exact_match, num_threads=50, devset=updated_valset, display_progress=True, max_errors=500, return_outputs=True)\n",
    "gpt_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 86 / 201  (42.8):  25%|██▍       | 200/805 [00:20<00:35, 17.05it/s]2024/11/13 00:28:49 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 366.0 / 805  (45.5): 100%|██████████| 805/805 [01:02<00:00, 12.81it/s]\n",
      "2024/11/13 00:29:32 INFO dspy.evaluate.evaluate: Average Metric: 366.0 / 805 (45.5%)\n",
      "Average Metric: 0 / 1  (0.0):   0%|          | 1/805 [00:01<26:11,  1.95s/it]2024/11/13 00:29:34 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 0.0 / 4  (0.0):   0%|          | 4/805 [00:02<05:15,  2.54it/s]2024/11/13 00:29:35 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 0.0 / 6  (0.0):   1%|          | 6/805 [00:02<04:11,  3.17it/s]2024/11/13 00:29:35 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 1.0 / 9  (11.1):   1%|          | 9/805 [00:03<04:18,  3.07it/s]2024/11/13 00:29:36 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 10.0 / 35  (28.6):   4%|▍         | 35/805 [00:12<02:58,  4.31it/s]2024/11/13 00:29:45 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 30.0 / 86  (34.9):  11%|█         | 86/805 [00:27<02:50,  4.21it/s]2024/11/13 00:30:00 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 46.0 / 119  (38.7):  15%|█▍        | 119/805 [00:34<02:25,  4.70it/s]2024/11/13 00:30:06 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 49.0 / 133  (36.8):  16%|█▋        | 132/805 [00:36<02:53,  3.88it/s]2024/11/13 00:30:09 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 49.0 / 136  (36.0):  17%|█▋        | 136/805 [00:36<01:41,  6.57it/s]2024/11/13 00:30:09 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 70.0 / 175  (40.0):  22%|██▏       | 175/805 [00:44<01:45,  5.99it/s]2024/11/13 00:30:18 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 76.0 / 195  (39.0):  24%|██▍       | 194/805 [00:50<02:21,  4.32it/s]2024/11/13 00:30:23 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 76.0 / 199  (38.2):  25%|██▍       | 198/805 [00:51<02:20,  4.33it/s]2024/11/13 00:30:24 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 81.0 / 219  (37.0):  27%|██▋       | 219/805 [00:57<02:01,  4.84it/s]2024/11/13 00:30:29 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 95.0 / 249  (38.2):  31%|███       | 249/805 [01:04<03:02,  3.04it/s]2024/11/13 00:30:37 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 101.0 / 260  (38.8):  32%|███▏      | 260/805 [01:08<03:44,  2.43it/s]2024/11/13 00:30:41 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 104.0 / 270  (38.5):  34%|███▎      | 270/805 [01:11<02:15,  3.96it/s]2024/11/13 00:30:43 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 104.0 / 272  (38.2):  34%|███▍      | 272/805 [01:11<01:41,  5.27it/s]2024/11/13 00:30:44 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 106.0 / 277  (38.3):  34%|███▍      | 277/805 [01:12<01:38,  5.35it/s]2024/11/13 00:30:44 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 147.0 / 355  (41.4):  44%|████▍     | 355/805 [01:32<02:24,  3.12it/s]2024/11/13 00:31:05 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 147.0 / 356  (41.3):  44%|████▍     | 356/805 [01:32<02:35,  2.88it/s]2024/11/13 00:31:05 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 152.0 / 364  (41.8):  45%|████▌     | 364/805 [01:38<04:59,  1.47it/s]2024/11/13 00:31:10 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 153.0 / 368  (41.6):  46%|████▌     | 368/805 [01:39<03:46,  1.93it/s]2024/11/13 00:31:13 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 156.0 / 373  (41.8):  46%|████▋     | 373/805 [01:42<03:38,  1.98it/s]2024/11/13 00:31:16 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 161.0 / 399  (40.4):  50%|████▉     | 399/805 [01:56<03:39,  1.85it/s]2024/11/13 00:31:29 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 165.0 / 407  (40.5):  50%|█████     | 406/805 [01:59<02:48,  2.36it/s]2024/11/13 00:31:33 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 166.0 / 411  (40.4):  51%|█████     | 411/805 [02:02<03:10,  2.07it/s]2024/11/13 00:31:35 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 166.0 / 412  (40.3):  51%|█████     | 412/805 [02:03<03:58,  1.65it/s]2024/11/13 00:31:35 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 167.0 / 422  (39.6):  52%|█████▏    | 422/805 [02:05<01:49,  3.49it/s]2024/11/13 00:31:38 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 228.0 / 523  (43.6):  65%|██████▍   | 523/805 [02:28<00:57,  4.94it/s]2024/11/13 00:32:00 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 239.0 / 538  (44.4):  67%|██████▋   | 538/805 [02:31<00:54,  4.88it/s]2024/11/13 00:32:04 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 250.0 / 562  (44.5):  70%|██████▉   | 561/805 [02:39<02:04,  1.96it/s]2024/11/13 00:32:12 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 250.0 / 564  (44.3):  70%|███████   | 564/805 [02:40<01:34,  2.55it/s]2024/11/13 00:32:13 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 252.0 / 567  (44.4):  70%|███████   | 567/805 [02:42<01:49,  2.17it/s]2024/11/13 00:32:14 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 253.0 / 572  (44.2):  71%|███████   | 572/805 [02:44<02:05,  1.85it/s]2024/11/13 00:32:17 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 253.0 / 573  (44.2):  71%|███████   | 573/805 [02:45<02:11,  1.76it/s]2024/11/13 00:32:18 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 262.0 / 594  (44.1):  74%|███████▍  | 594/805 [02:56<02:56,  1.20it/s]2024/11/13 00:32:29 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 262.0 / 598  (43.8):  74%|███████▍  | 598/805 [02:59<02:42,  1.27it/s]2024/11/13 00:32:32 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 264.0 / 604  (43.7):  75%|███████▌  | 604/805 [03:03<02:17,  1.47it/s]2024/11/13 00:32:36 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 273.0 / 628  (43.5):  78%|███████▊  | 628/805 [03:17<02:10,  1.36it/s]2024/11/13 00:32:52 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 279.0 / 637  (43.8):  79%|███████▉  | 637/805 [03:23<01:19,  2.11it/s]2024/11/13 00:32:56 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 279.0 / 639  (43.7):  79%|███████▉  | 639/805 [03:24<01:17,  2.15it/s]2024/11/13 00:32:57 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 279.0 / 641  (43.5):  80%|███████▉  | 641/805 [03:25<01:03,  2.60it/s]2024/11/13 00:32:58 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 282.0 / 652  (43.3):  81%|████████  | 652/805 [03:29<00:51,  2.96it/s]2024/11/13 00:33:02 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 286.0 / 665  (43.0):  82%|████████▏ | 664/805 [03:32<00:36,  3.82it/s]2024/11/13 00:33:05 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 289.0 / 672  (43.0):  83%|████████▎ | 671/805 [03:34<00:36,  3.65it/s]2024/11/13 00:33:07 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 294.0 / 679  (43.3):  84%|████████▍ | 679/805 [03:36<00:31,  3.99it/s]2024/11/13 00:33:09 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 296.0 / 686  (43.1):  85%|████████▌ | 686/805 [03:40<00:53,  2.20it/s]2024/11/13 00:33:13 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 309.0 / 708  (43.6):  88%|████████▊ | 708/805 [03:48<00:24,  4.02it/s]2024/11/13 00:33:21 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 313.0 / 720  (43.5):  89%|████████▉ | 720/805 [03:52<00:24,  3.51it/s]2024/11/13 00:33:25 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 319.0 / 731  (43.6):  91%|█████████ | 731/805 [03:56<00:24,  3.05it/s]2024/11/13 00:33:29 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 324.0 / 739  (43.8):  92%|█████████▏| 739/805 [03:58<00:17,  3.88it/s]2024/11/13 00:33:31 ERROR dspy.evaluate.evaluate: Error for example in dev set: \t\t Images are not yet supported in JSON mode.. Set `provide_traceback=True` to see the stack trace.\n",
      "Average Metric: 356.0 / 805  (44.2): 100%|██████████| 805/805 [04:18<00:00,  3.12it/s]\n",
      "2024/11/13 00:33:50 INFO dspy.evaluate.evaluate: Average Metric: 356.0 / 805 (44.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMMU Validation Set (single image only, multiple choice only), N=805\n",
      "Temp 0, max_tokens=5k\n",
      "qwen-7b\n",
      "Reported: 54.1\n",
      "gpt-4o-mini\n",
      "Reported: 59.4\n",
      "Measured (cot, predict): 44.2, 45.47\n",
      "Num bad format (cot, predict): 51, 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_lm(lm, cot=False):\n",
    "    if lm.model == \"openai/gpt-4o-mini\":\n",
    "        num_threads = 10\n",
    "    else:\n",
    "        num_threads = 30\n",
    "    evaluate_mmmu = Evaluate(metric=answer_exact_match, num_threads=num_threads, devset=updated_valset, display_progress=True, max_errors=500, return_outputs=True)\n",
    "    mmmu = MMMUModule(cot=cot)\n",
    "    with dspy.context(lm=lm):\n",
    "        scores, outputs = evaluate_mmmu(mmmu)\n",
    "        num_bad_format = sum(1 for example in outputs if example[1].get(\"answer\", None) is None)\n",
    "        return scores, num_bad_format\n",
    "\n",
    "# res1 = test_lm(qwen_lm)\n",
    "# res1_cot = test_lm(qwen_lm, cot=True)\n",
    "# test_lm(haiku_lm)\n",
    "# test_lm(llama_lm)\n",
    "# res1 = test_lm(internlm_lm)\n",
    "res2 = test_lm(gpt_lm)\n",
    "res2_cot = test_lm(gpt_lm, cot=True)\n",
    "# Results:\n",
    "# MMMU Val(single image only, multiple choice only), N=805\n",
    "# Temp 0, max_tokens=5k\n",
    "\n",
    "# 4o-mini:\n",
    "# Reported: 59.4\n",
    "\n",
    "# Measured (cot, predict): 60.0, 56.4\n",
    "# Num bad format (cot, predict): 0, 1\n",
    "\n",
    "# qwen-7b\n",
    "# Reported: 54.1\n",
    "# Measured (cot, predict): 49.0, 49.69\n",
    "# Num bad format (cot, predict): 17, 0\n",
    "print(\"MMMU Validation Set (single image only, multiple choice only), N=805\")\n",
    "print(\"Temp 0, max_tokens=5k\")\n",
    "# print(\"qwen-7b\")\n",
    "# print(\"Reported:\", 54.1)\n",
    "# print(\"Measured (cot, predict):\", f\"{res1_cot[0]:.1f}, {res1[0]:.2f}\")\n",
    "# print(\"Num bad format (cot, predict):\", f\"{res1_cot[1]}, {res1[1]}\")\n",
    "# print()\n",
    "print(\"gpt-4o-mini\")\n",
    "print(\"Reported:\", 59.4)\n",
    "print(\"Measured (cot, predict):\", f\"{res2_cot[0]:.1f}, {res2[0]:.2f}\") \n",
    "print(\"Num bad format (cot, predict):\", f\"{res2_cot[1]}, {res2[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2024-11-13T00:33:50.703870]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "2. `image` (Image)\n",
      "3. `choices` (list[str])\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `answer` (Literal[A, B, C, D, E])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## image ## ]]\n",
      "{image}\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "{choices}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}        # note: the value you produce must be one of: A; B; C; D; E\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer with the letter of the correct answer.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Some controversial groups that may be mislabelled as <image 1> include:\n",
      "\n",
      "[[ ## image ## ]]\n",
      "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1080x746 at 0x7921E40DD150>\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "['A. Scientology and the Hare Krishna', \"B. the Peoples Temple and Heven's Gate\", 'C. the Branch Davidians and the Manson Family', 'D. Quakers and Petecostals']\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## answer ## ]]` (must be formatted as a valid Python Literal[A, B, C, D, E]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The image likely represents a controversial group or cult, and the options provided include various groups that have been labeled as such. Scientology and the Hare Krishna movement are often viewed as controversial due to their beliefs and practices, but they are not typically mislabelled as cults in the same way as the other groups listed. The Peoples Temple and Heaven's Gate are both infamous for their tragic endings and are often categorized as cults. The Branch Davidians and the Manson Family are also widely recognized as controversial groups. In contrast, Quakers and Pentecostals are mainstream religious groups and are not typically associated with the term \"cult.\" Therefore, the most fitting answer that aligns with the idea of being mislabelled as a cult would be option B, as both groups have been involved in significant controversies and are often discussed in the context of cult-like behavior.\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "B\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, outputs = evaluate_mmmu(mmmu)\n",
    "# lm.inspect_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([outputs[i][1].get(\"answer\", \"nothing returned\") for i in range(len(outputs))])\n",
    "non_letters = sum([1 for output in outputs if output[1].get(\"answer\", \"nothing returned\") not in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "print(c)\n",
    "print(non_letters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_correct = sum(outputs[i][2] for i in range(len(outputs)) if outputs[i][0][\"question_type\"] == \"multiple-choice\")\n",
    "total_mc = sum(1 for example in outputs if example[0][\"question_type\"] == \"multiple-choice\")\n",
    "print(mc_correct, total_mc)\n",
    "print(mc_correct / total_mc)\n",
    "print(sum(outputs[i][1].get(\"answer\", None) is None for i in range(len(outputs))))\n",
    "\n",
    "# Note: Run above here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that multiple images work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "def set_image_to_black_square(example, key):\n",
    "    example_copy = example.copy()\n",
    "    example_copy[key] = PIL.Image.open(\"black_image_300x300.png\")\n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "print(updated_devset[0][\"image_1\"])\n",
    "print(updated_devset[0][\"image_2\"])\n",
    "examples_no_image_1 = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "print(examples_no_image_1[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_1[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "examples_no_image_2 = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), updated_valset))\n",
    "print(examples_no_image_2[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_2[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), examples_no_actual_image))\n",
    "print(examples_no_actual_image[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_actual_image[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1[0].inputs())\n",
    "print(mmmu(**examples_no_image_1[0].inputs()))\n",
    "\n",
    "print(examples_no_image_2[0].inputs())\n",
    "print(mmmu(**examples_no_image_2[0].inputs()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=updated_valset)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Test with bootstrapped examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that JPGs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to JPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_jpg(example):\n",
    "    example_copy = example.copy()\n",
    "    for key in ['image_1', 'image_2']:\n",
    "        if key in example_copy and isinstance(example_copy[key], Image.Image):\n",
    "            # Convert to RGB mode (in case it's not already)\n",
    "            img = example[key].convert('RGB')\n",
    "            \n",
    "            # Save as JPG in memory\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG')\n",
    "            buffer.seek(0)\n",
    "            \n",
    "            # Load the JPG back as a PIL Image\n",
    "            example_copy[key] = Image.open(buffer)\n",
    "    \n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "# Convert all images in the dataset to JPG\n",
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "\n",
    "# Verify conversion\n",
    "print(\"Original image format:\", updated_valset[0]['image_1'].format)\n",
    "print(\"Converted image format:\", examples_jpg[0]['image_1'].format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "examples_no_image_1_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_1))\n",
    "examples_no_image_2_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_2))\n",
    "examples_no_actual_image_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_actual_image))\n",
    "\n",
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1_jpg[0].inputs())\n",
    "print(mmmu(**examples_no_image_1_jpg[0].inputs()))\n",
    "print(examples_no_image_1_jpg[0][\"image_1\"].format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=examples_jpg)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1_jpg)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2_jpg)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image_jpg)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that URLs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    \"White\": \"FFFFFF\",\n",
    "    \"Red\": \"FF0000\",\n",
    "    \"Green\": \"00FF00\",\n",
    "    \"Blue\": \"0000FF\",\n",
    "    \"Yellow\": \"FFFF00\",\n",
    "    \"Cyan\": \"00FFFF\",\n",
    "    \"Magenta\": \"FF00FF\",\n",
    "    \"Gray\": \"808080\",\n",
    "    \"Orange\": \"FFA500\",\n",
    "    \"Purple\": \"800080\"\n",
    "}\n",
    "def get_color_image_url(color, file_extension=\"png\"):\n",
    "    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_2_color_image_examples(n):\n",
    "    examples = []\n",
    "    for _ in range(n):\n",
    "        color_1, color_2 = random.sample(list(colors.keys()), 2)\n",
    "        chosen_color = color_1 if random.random() < 0.5 else color_2\n",
    "        chosen_image = \"image_1\" if chosen_color == color_1 else \"image_2\"\n",
    "        example_kwargs = {\n",
    "            \"image_1\": get_color_image_url(color_1),\n",
    "            \"image_2\": get_color_image_url(color_2),\n",
    "            \"question\": f\"What color is {chosen_image}?\",\n",
    "            \"answer\": chosen_color\n",
    "        }\n",
    "        examples.append(dspy.Example(**example_kwargs).with_inputs(\"image_1\", \"image_2\", \"question\"))\n",
    "    return examples\n",
    "\n",
    "examples = generate_random_2_color_image_examples(100)\n",
    "print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorSignature(dspy.Signature):\n",
    "    \"\"\"Output the color of the designated image.\"\"\"\n",
    "    image_1: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    image_2: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    question: str = dspy.InputField(desc=\"A question about the image\")\n",
    "    answer: str = dspy.OutputField(desc=\"The color of the designated image\")\n",
    "color_program = dspy.Predict(ColorSignature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(examples[0])\n",
    "print(color_program(**examples[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=3, max_labeled_demos=10)\n",
    "smaller_few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=1, max_labeled_demos=1)\n",
    "dataset = generate_random_2_color_image_examples(1000)\n",
    "trainset = dataset[:200]\n",
    "validationset = dataset[200:400]\n",
    "evaluate_colors = Evaluate(metric=answer_exact_match, num_threads=300, devset=validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_color_program = few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "compiled_smaller_color_program = smaller_few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "print(evaluate_colors(color_program))\n",
    "print(evaluate_colors(compiled_color_program))\n",
    "print(evaluate_colors(compiled_smaller_color_program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compiled_color_program(**validationset[0].inputs()))\n",
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO(Isaac): Delete; Archive of old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader().from_huggingface(\"Alanox/stanford-dogs\", split=\"full\", input_keys=(\"image\",), trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the field from \"image\" to \"image_1\"\n",
    "def rename_field(example, old_name, new_name):\n",
    "    try:\n",
    "        example[new_name] = example[old_name]\n",
    "        del example[old_name]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return example\n",
    "    \n",
    "dog_dataset = list(map(rename_field, dataset, [\"image\"]*len(dataset), [\"image_1\"]*len(dataset)))\n",
    "dog_dataset2 = list(map(rename_field, dog_dataset, [\"target\"]*len(dog_dataset), [\"answer\"]*len(dog_dataset)))\n",
    "dog_dataset3 = list(map(lambda x: x.with_inputs(\"image_1\"), dog_dataset2))\n",
    "dog_dataset = dog_dataset3\n",
    "random.shuffle(dog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogPictureSignature(dspy.Signature):\n",
    "    \"\"\"Output the dog breed of the dog in the image.\"\"\"\n",
    "    image: dspy.Image = dspy.InputField(desc=\"An image of a dog\")\n",
    "    answer: str = dspy.OutputField(desc=\"The dog breed of the dog in the image\")\n",
    "\n",
    "class DogPicture(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.predictor = dspy.ChainOfThought(DogPictureSignature)\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n",
    "\n",
    "dog_picture = DogPicture()\n",
    "\n",
    "example = dspy.Example(image=dspy.Image.from_url(\"https://i.pinimg.com/564x/78/f9/6d/78f96d0314d39a1b8a849005123e166d.jpg\"))\n",
    "print(dog_picture(**example.inputs()))\n",
    "# print(dog_picture(**dog_dataset[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test inline signature\n",
    "# TODO: Test json adapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
